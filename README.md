# awesome-knowledge-distillation [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

Hello visitors, I have been interested in an efficient deep neural network design, such as pruning, AutoML, quantization, and focused on the knowledge distillation for successful network generalization. This page organizes for the knowledge distillation.

## History

- [2021 years](#2021)
- [2020 years](#2020)
- [2019 years](#2019)
- [2018 years](#2018)
- [2017 years](#2017)
- [2016 years](#2016)
- [2015 years](#2015)


### 2021
|   Title  | Issue | Release |
| :--------| :---: | :-----: |
| [Data-free Knowledge Distillation for Object Detection](https://openaccess.thecvf.com/content/WACV2021/papers/Chawla_Data-Free_Knowledge_Distillation_for_Object_Detection_WACV_2021_paper.pdf) | WACV | - |

### 2020
|   Title  | Issue | Release |
| :--------| :---: | :-----: |
| [Fakd: Feature-Affinity Based Knowledge Distillation for Efficient Image Super-Resolution](https://ieeexplore.ieee.org/abstract/document/9190917) | ICIP | - |
| [Feature Fusion for Online Mutual Knowledge Distillation](https://arxiv.org/pdf/1904.09058.pdf) | ICPR | - |
| [Residual Distillation: Towards Portable Deep Neural Networks without Shortcuts](https://papers.nips.cc/paper/2020/file/657b96f0592803e25a4f07166fff289a-Paper.pdf) | NeurIPS | - |
| [Knowledge Distillation in Wide Neural Networks: Risk Bound, Data Efficiency and Imperfect Teacher](https://papers.nips.cc/paper/2020/file/ef0d3930a7b6c95bd2b32ed45989c61f-Paper.pdf) | NeurIPS | - |
| [Agree to Disagree: Adaptive Ensemble Knowledge Distillation in Gradient Space](https://papers.nips.cc/paper/2020/file/91c77393975889bd08f301c9e13a44b7-Paper.pdf) | NeurIPS | [GitHub](https://github.com/AnTuo1998/AE-KD.) |
| [Local correlation consistency for knowledge distillation](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123570018.pdf) | ECCV | - |
| [Online Knowledge Distillation via Collaborative Learning](https://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Online_Knowledge_Distillation_via_Collaborative_Learning_CVPR_2020_paper.pdf) | CVPR | - |
| [Explaining Knowledge Distillation by Quantifying the Knowledge](https://openaccess.thecvf.com/content_CVPR_2020/papers/Cheng_Explaining_Knowledge_Distillation_by_Quantifying_the_Knowledge_CVPR_2020_paper.pdf) | CVPR | - |
| [Revisiting Knowledge Distillation via Label Smoothing Regularization](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yuan_Revisiting_Knowledge_Distillation_via_Label_Smoothing_Regularization_CVPR_2020_paper.pdf) | CVPR | [GitHub](https://github.com/yuanli2333/Teacher-free-Knowledge-Distillation) |
| [Contrastive Representation Distillation](https://arxiv.org/abs/1910.10699) | ICLR | [Github](https://github.com/HobbitLong/RepDistiller) |
| [Ensemble Distribution Distillation](https://openreview.net/pdf?id=BygSP6Vtvr) | ICLR | - |
| [Online Knowledge Distillation with Diverse Peers](https://aaai.org/Papers/AAAI/2020GB/AAAI-ChenD.4552.pdf) | AAAI | [Github](https://github.com/DefangChen/OKDDip-AAAI2020) |


### 2019
|   Title  | Issue | Release |
| :--------| :---: | :-----: |
| [Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons](https://arxiv.org/abs/1811.03233) | AAAI | [Github](https://github.com/bhheo/AB_distillation) |
| [A Comprehensive Overhaul of Feature Distillation](https://arxiv.org/abs/1904.01866) | ICCV | [Github](https://github.com/clovaai/overhaul-distillation) |
| [Similarity-Preserving Knowledge Distillation](https://arxiv.org/abs/1907.09682) | ICCV | - |
| [Diversity with Cooperation: Ensemble Methods for Few-Shot Classification](http://openaccess.thecvf.com/content_ICCV_2019/papers/Dvornik_Diversity_With_Cooperation_Ensemble_Methods_for_Few-Shot_Classification_ICCV_2019_paper.pdf) | ICCV | [Github](https://github.com/dvornikita/fewshot_ensemble) |
| [Knowledge Distillation via Instance Relationship Graph](http://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_Knowledge_Distillation_via_Instance_Relationship_Graph_CVPR_2019_paper.pdf) | CVPR | [Github](https://github.com/yufanLIU/IRG) |
| [Relational Knowledge Distillation](https://arxiv.org/abs/1904.05068) | CVPR | [Github](https://github.com/lenscloth/RKD) |
| [Correlation congruence for knowledge distillation](https://openaccess.thecvf.com/content_ICCV_2019/papers/Peng_Correlation_Congruence_for_Knowledge_Distillation_ICCV_2019_paper.pdf) | ICCV | - |

### 2018
|   Title  | Issue | Release |
| :--------| :---: | :-----: |
| [Born-Again Neural Networks](https://arxiv.org/abs/1805.04770) | ICML | - |
| [Knowledge Transfer with Jacobian Matching](https://arxiv.org/abs/1803.00443) | ICML | - |
| [Paraphrasing Complex Network: Network Compression via Factor Transfer](https://papers.nips.cc/paper/7541-paraphrasing-complex-network-network-compression-via-factor-transfer) | NeurIPS | - |
| [Knowledge Distillation by On-the-Fly Native Ensemble](https://papers.nips.cc/paper/7980-knowledge-distillation-by-on-the-fly-native-ensemble.pdf) | NeurIPS | [Github](https://github.com/Lan1991Xu/ONE_NeurIPS2018) |
| [Collaborative Learning for Deep Neural Networks](https://papers.nips.cc/paper/7454-collaborative-learning-for-deep-neural-networks.pdf) | NeurIPS | - |
| [Deep Mutual Learning](https://zpascal.net/cvpr2018/Zhang_Deep_Mutual_Learning_CVPR_2018_paper.pdf) | CVPR | [Github](https://github.com/chxy95/Deep-Mutual-Learning) |
| [Large Scale Distributed Neural Network Training Through Online Distillation](https://openreview.net/pdf?id=rkr1UDeC-) | ICLR | - |
| [Label Refinery: Improving ImageNet Classification through Label Progression](https://arxiv.org/abs/1805.02641) | Preprint | [Github](https://github.com/hessamb/label-refinery) |



### 2017
|   Title  | Issue | Release |
| :--------| :---: | :-----: |
| [Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer](https://arxiv.org/abs/1612.03928) | ICLR | [Github](https://github.com/szagoruyko/attention-transfer) |
| [A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning](http://openaccess.thecvf.com/content_cvpr_2017/papers/Yim_A_Gift_From_CVPR_2017_paper.pdf) | CVPR | - |

### 2015
|   Title  | Issue | Release |
| :--------| :---: | :-----: |
| [FitNets: Hints for Thin Deep Nets](https://arxiv.org/abs/1412.6550) | ICLR | [Github](https://github.com/adri-romsor/FitNets) |
| [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531) | NeurIPS | - |
